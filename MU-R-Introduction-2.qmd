---
title: "R-Introduction - Part 2"
subtitle: "A Workshop for Modul University Vienna Faculty"
author: Gunther Maier
toc: true
number-sections: true
bibliography: bibliography.bib
format: 
  pdf: 
    papersize: a4
    pdf-engine: pdflatex
#  html:
#    theme: default
##    toc_float: true
---

{{< pagebreak >}}

# Introduction

This is the second part of a very brief introduction to R. I compiled this information for a presentation to faculty at Modul University Vienna.

In this part I assume that you are familiar with the topics we covered in the first workshop. This includes topics like - the difference between R and RStudio - The main parts of RStudio - What are packages and where can I get them? - How to install a package - How to get help

I will repeat the parts we did about loading data.

{{< pagebreak >}}

# Loading data

## Using data available in R and R packages

R comes with a set of data-sets that are handy for testing and demonstration purposes. The data() command without parameters generates a list of all the datasets (the output goes to a separate window and therefore does not show in Quarto).

```{r}
#| eval: false
data()
```

If you want to see **all** datasets available in **all** installed packages, use the following command. It is mentioned at the bottom of the output of "data()". Again, the output does not show in Quarto,

```{r}
#| eval: false
data(package = .packages(all.available = TRUE))
```

One of the datasets available by default (in the standard package "datasets") is "mtcars". To use this dataset in our analysis, we type

```{r}
data("mtcars")
```

This loads the dataset "mtcars" into the environment. Check the Environment pane in the Environment window. You should see an entry named "mtcars".

The data is now available as a dataframe in your R session. To investigate the data, we can list the first six lines with

```{r}
head(mtcars)
```

In one webpage we saw that there is also a dataset "mpg" with information about fuel consumption of various cars. We try to load this datase with

```{r}
data("mpg")
```

This command generates an error message. The reason is that the package "ggplot2", which contains this dataset is not available in our R session yet. If we do not want to load this package, we can set the "package" parameter in the data() command:

```{r}
data(mpg, package="ggplot2")
```

Alternatively, we can first load the package and then the dataset.

```{r}
library(ggplot2)
data(mpg)
```

Again, we list the head of the dataframe.

```{r}
head(mpg)
```

## Importing an Excel-file

We first have to load the package `readxl`. The package contains a function `read_excel`. Take a look at the description of this function through `?read_excel`.

```{r}
library(readxl)
```

To demonstrate and test, we need data in Excel format. Let us use the Austrian results of the EU-elections 2014. Go to the webpage <https://www.data.gv.at/>, and select "Daten" - "Datensatz finden" (you may want to witch to English and then select "data" - "find dataset"). Enter "EU Wahl 2014" to the search field and hit Enter. Scroll down to "Ergebnisse der EU-Wahl 2014 (BMI)" and click the green button with the white "X". This will download the Excel-file to your download directory. Move or copy the file to your project directory.

Now, run

```{r}
EU_elections <- read_excel(
  "eu-wahl2014-endgueltiges_ergebnis_mit_briefwahl.xlsx")
```

R alerts us that some of the variables were given new names. To view the first six lines and five columns of the dataset, enter

```{r}
head(EU_elections, c(6, 5))
```

The parameter `c(6, 5)` sets the number of rows to display to 6 and the number of columns to 5.

Alternatively, you may want to load the whole data frame into the viewer with

```{r}
#| eval: false
View(EU_elections)
```

When we look at the data, we find that there are some problems. Underneath the variable names, we see a row with mainly "NA" values. In columns 5-7 we see that there seems to be a second row for the variable header. Because of the strings in these fields, all the values in columns 5-7 are stored as characters (indicated by the "<chr>").

To avoid these problems, we can tell the function `read_excel()` to skip the first two lines.

```{r}
#| code-overflow: wrap
EU_elections <- read_excel(
  "eu-wahl2014-endgueltiges_ergebnis_mit_briefwahl.xlsx",
  skip=2)
head(EU_elections, c(6, 7))
```

Now, we have the problem that the first line of data is used as variable names. So, we also tell the function to **not** read column names

```{r}
#| code-overflow: wrap
EU_elections <- read_excel(
  "eu-wahl2014-endgueltiges_ergebnis_mit_briefwahl.xlsx",
  skip=2, 
  col_names = FALSE)
head(EU_elections, c(6, 7))
```

Instead of `col_names = FALSE`, we can also provide a vector of variable names to the parameter `col_names`. This will give us a nice data frame to use.

```{r}
EU_elections <- read_excel(
  "eu-wahl2014-endgueltiges_ergebnis_mit_briefwahl.xlsx",
  skip=2, 
  col_names = c("GKZ", "Name", "Voters", "Turnout", "Votes", "invalid", 
                "valid", "ÖVP", "ÖVP_percent", "SPÖ", "SPÖ_percent", 
                "FPÖ", "FPÖ_percent", "GRÜNE", "GRÜNE_percent", "BZÖ", 
                "BZÖ_percent", "NEOS", "NEOS_percent", "RECOS", 
                "RECOS_percent", "ANDERS", "ANDERS_percent", "EUSTOP",
                "EUSTOP_percent"))
head(EU_elections, c(6, 7))
```

## Importing a CSV-file directly from the Internet

We pack two new topics under this heading: (1) reading CSV-files and (2) reading data directly from the Internet.

CSV is a common data format and stands for "Comma Separated Variables". This name, however, is misleading because in German speaking countries it is usually the semicolon (;), not the comma that separates variables.

R provides two functions for reading a CSV file: `read.csv()` and `read.csv2()`. The first version uses English standards (comma as variable separator and period as decimal separator), the second version uses German standards (semicolon as variable separator and comma as decimal separator). In both cases, you can define the respective separators with the parameters `sep=` and `dec=`.

Some data import functions in R allow you to specify a url instead of a local filepath for the file. The two CSV-functions are among them, `read_excel()`is not. When you go back to the <https://www.data.gv.at/> webpage, to the entry "Ergebnisse der EU-Wahl 2014 (BMI)", you will see an orange icon marked "CSV". Right-click this icon and select Copy Link to copy the url into your computer's clipboard. Paste this url into the following call of `read.csv2()`.

```{r}
#| eval: false
csvurl <- paste("https://www.data.gv.at/katalog/dataset/", 
                "2b10a91b-51d5-4e34-b992-8fd3a3121f0d/resource/",
                "a5ccea30-a505-4376-bfa3-fa2585c69192/download/",
                "eu-wahl2014-endgueltiges_ergebnis_mit_briefwahl.csv", 
            sep="")
EU_elections_csv <- read.csv2(csvurl)
```

This does not work. The reason is again the issue with the column names, which are not unique. We can again skip the first two lines with `skip = 2`. Unfortunately, this function does not allow us to provide variable names as before. We can only set `header = FALSE` to prevent the function from interpreting the first line as variable names.

```{r}
csvurl <- paste("https://www.data.gv.at/katalog/dataset/", 
                "2b10a91b-51d5-4e34-b992-8fd3a3121f0d/resource/",
                "a5ccea30-a505-4376-bfa3-fa2585c69192/download/",
                "eu-wahl2014-endgueltiges_ergebnis_mit_briefwahl.csv", 
            sep="")
EU_elections_csv <- read.csv2(csvurl, 
    skip=2, 
    header = FALSE)
head(EU_elections_csv, c(6, 7))
```

Note that the "ü" in "Burgenland Süd" is not read correctly. This issue is related to the import of the file from the Internet. German Umlaut characters are treated differently in various character systems, socalled "encodings". To fix this, we need to use the parameter `encoding` and specify the correct encoding. An alternative fix would be to download the file to the local machine and then read it into R in a second step.

We use the following code to read the file with the correct encoding:

```{r}
csvurl <- paste("https://www.data.gv.at/katalog/dataset/", 
                "2b10a91b-51d5-4e34-b992-8fd3a3121f0d/resource/",
                "a5ccea30-a505-4376-bfa3-fa2585c69192/download/",
                "eu-wahl2014-endgueltiges_ergebnis_mit_briefwahl.csv", 
            sep="")
EU_elections_csv <- read.csv2(csvurl, 
    skip=2, 
    header = FALSE,
    encoding = "latin1")
head(EU_elections_csv, c(6, 7))
```

Now the Umlaut characters are displayed correctly.

With `read.csv2()`, we cannot specify the variable names and have to set the correct names in a separate step. With the function `colnames()` we can get and set the column names of a data frame.

```{r}
colnames(EU_elections_csv) <- c("GKZ", "Name", "Voters", "Turnout", 
      "Votes", "invalid", "valid", "ÖVP", "ÖVP_percent", "SPÖ", 
      "SPÖ_percent", "FPÖ", "FPÖ_percent", "GRÜNE", "GRÜNE_percent", 
      "BZÖ", "BZÖ_percent", "NEOS", "NEOS_percent", "RECOS", 
      "RECOS_percent", "ANDERS", "ANDERS_percent", "EUSTOP",
      "EUSTOP_percent")
head(EU_elections_csv, c(6, 7))
```

{{< pagebreak >}}

# Viewing data

In any statistical software, it is always a good idea, to check and validate your data before you use it. That the routine you used to read the data does not generate an error message, does not imply that your data has been read *correctly*. Some data may be distorted, read in the wrong format, some numbers may end up in the wrong variables, etc.

I do not want to go into depth about checking and verifying data. At this stage, I just want to collect a few tools that you can use to look at your data.

## `head()`, `tail()`, and `data.table()`

We have already seen the command `head()` at work above. By default, it shows the first 6 rows for all the variables in the data frame. When the variables do not fit horizontally, the function prints them in blocks underneath one another. This output may be somewhat confusing and difficult to read. As you saw above, you can explicitly specify the numbers of rows and columns to display.

The command `tail()` does the same as `head()`, but with the last six lines of the dataframe. The function `data.table()`, which is available in the package "data.table", combines the functionality of `head()` and `tail()` and shows the first and the last six lines of the dataset.

## `View()`

The command `View()` (note the capital "V") displays the data frame in a spreadsheet like form. The output is placed in a new tab in the Source window of RStudio. This is probably the best option to inspect your data because you can scroll through all the rows and columns. The function may fail, however, when you have to deal with a very large data frame.

When you move the cursor over the name of a column, you will see more information about this variable in a tooltip. Run `View(EU_elections)`, place the cursor over "Turnout", and after a second you will see that this is column 4 of the data frame, that the format is "numeric" and that the numbers are in the range "0.1 - 0.8".

You can call this function also from the Environment pane of the Environment window. To the right of every listed data frame there is a light-blue icon with a data grid. Clicking that will load this data frame into `View()`.

## The glimpse command

Somewhere between `head()` and `tail()` on the one hand and `View()` on the other is the `glimpse()` command from the package "dplyr". This package is loaded automatically when you load the package "tidyverse" (see below).

```{r}
library(dplyr)
glimpse(EU_elections)
```

This function displays the size of the data frame (number of rows and number of columns) and then for every variable its name, its data type, and the first few values (whatever fits into the available horizontal space).

## Nicer looking tables

There are a number of packages that provide functions for nicely formatted tables. One example is the function `gt()` from the "gt" package. Try it out with the following code.

```{r}
library(gt)
gt(EU_elections[1:8, 1:7])
```

I added `[1:8, 1:7]` to the name of the data frame to restrict the table to the first eight rows and the first seven columns. This is solely for display purposes for the PDF-output. When you leave this out and use `gt(EU_elections)`, the whole data frame will be shown in the table.

When you call this function from the Console, the output will show up in the Viewer pane of the Files window. Click the Zoom button to get a larger view in a separate window. With the Export button you can also save the table as a graphic, as a webpage, or copy it to the clipboard to paste it into another program.

{{< pagebreak >}}

# Managing data

When we have read our data into R, we usually have to do some data manipulations before we can run some analysis. For these tasks, we will use the package "tidyverse" that is quite popular for these tasks. One advantage of the data-management functions in this package is that these functions can be piped with the result of one function being sent into the next one. We will see this in action below.

For this section, I follow the video "Data wrangling with R in 27 minutes" by Andrew Gard, a professor of mathematics and computer science at Lake Forest College, in Lake Forest IL, USA (<https://youtu.be/oXImkptBpqc?si=7tk9yqxUoI9I-ooX>).

Before we continue, let us first load the package "tidyverse" (this produces some output which I suppress here):

```{r}
#| output: false

library(tidyverse)
```

When you load this package, you are actually loading a set of sub-packages. The functions that we will use in this section are all in the package "dplyr". Instead of "tidyverse" you could actually just load this package and still get the functions that I will discuss here.

Another important sub-package of "tidyverse" is "ggplot2", an innovative package for plotting variables.

## Selecting rows

The dataframe "EU_elections" that we loaded before, contains information about all communes in Austria. In a first step, suppose, we are only interested in the results for Lower Austria and want to extract those from the data frame.

When we inspect the data we see that for all communes in Lower Austria the first variable (GKZ) starts with "G3". So, we want to select those rows where "GKZ" starts with "G3". We do this with the function `filter()`. We can use this function in the standard form by giving it the input data frame and a condition for the filtering. For the condition, we use the function `str_starts()` and pass it the name of the variable to investigate and the string that we want the variable to start with. The following function returns "TRUE" when the variable "GKZ" starts with "G3" and FALSE otherwise.

```{r}
#| eval: false
str_starts(GKZ, "G3") 
```

So, our filter is as follows. We put the result into the data frame "tst".

```{r}
tst <- filter(EU_elections, str_starts(GKZ, "G3"))
```

Load the result into the viewer with `View(tst)`. You should only see data for Lower Austria.

Using piping, we can achieve exactly the same result with

```{r}
tst <- EU_elections %>% 
  filter(str_starts(GKZ, "G3"))
```

Here, we first determine the data frame and then pipe that to `filter()` with the piping command `%>%`. In RStudio, the keyboard shortcut for the piping command is "Ctrl-Shift-M". Note that we omit the name of the data frame in the `filter()` command.

In this simple example, the benefits of piping are not so obvious. They may become clearer when we want to use an additional filter. Say, we want to use only those observations from Lower Austria where the number of voters is larger than 10,000. In the standard notation, we can achieve this with a second condition in the `filter()` command:

```{r}
tst <- filter(EU_elections, str_starts(GKZ, "G3"), Voters > 10000)
```

With piping, we may use the result of the first `filter()`command and pipe it into a second one.

```{r}
tst <- EU_elections %>% 
  filter(str_starts(GKZ, "G3")) %>% 
  filter(Voters > 10000)
```

When we look at the result, we see that our filter also returns aggregates (districts, regions, the whole state). To eliminate those, we pipe the result through another filter. Now, we check whether "GKZ" ends with "00". This characterizes the aggregates. Since we want to *eliminate* those, we have to negate the result of `str_ends` with an exclamation mark ("!").

```{r}
tst <- EU_elections %>% 
  filter(str_starts(GKZ, "G3")) %>% 
  filter(Voters > 10000) %>% 
  filter(!str_ends(GKZ, "00"))
```

This produces the expected output and also demonstrates the value of piping. The R code is much better readable than when it is written in the standard notation.

## Selecting columns

To select columns from the data frame, we use the command `select()`. To illustrate this command, say, we want to extract only the columns "GKZ", "Name", "Voters", "Votes" and "valid". We do this in the following way

```{r}
tst <- select(EU_elections, "GKZ", "Name", "Voters", "Votes", "valid")
```

Alternatively, we can also identify the columns by number. Above, we selected columns 1 to 3, 5, and 7. Therefore, the above statement is equivalent to

```{r}
tst <- select(EU_elections, 1:3, 5, 7)
```

View the results of both versions to verify that they are identical and only contain these columns. Try to generate a data frame with only these columns and only communes in Lower Austria with more than 10,000 voters.

There are a number of helper functions available that allow us to identify columns by name. Suppose we want to exclude all the columns with "percent" in the name (all the percent variables). We could use the following command (it selects all columns whose name does not contain "percent").

```{r}
tst <- select(EU_elections, !contains("percent"))
```

Note the exclamation mark in front of `contains()`. We could also have used a minus sign and have said 'remove all columns that contain "percent"'.

## Arranging rows

Sometimes you need the observatons in a data frame in a certain order. Observations can be ordered with the `arrange()` command. All this command needs is the name of the variable by which to order. Suppose we want to see the data arranged by "GRÜNE_percent". We use this command:

```{r}
tst <- arrange(EU_elections, GRÜNE_percent)
```

In the results we see immediately that Tschanigraben in Burgenland has the lowest percentage (actually, the first eight communes all have no votes for this party). To find who has the highest percentage (the 7th district in Vienna), we have to scroll all the way to the bottom. If we want to arrange the observations in descending order of this variable, we just use a minus sign.

```{r}
tst <- arrange(EU_elections, -GRÜNE_percent)
```

The function `arrange()` can handle any number of variables and uses them from left to right to sort the observations. Say, we would like to order the observations for every State of Austria descending by the percentage of the party "GRÜNE". We could do this with the following command:

```{r}
tst <- arrange(EU_elections, substr(GKZ, 1, 2), -GRÜNE_percent)
```

We use the function `substr(GKZ, 1, 2)` to extract the first two characters from GKZ. They identify the "Bundesland". Since this is the first parameter after the name of the data frame, we first order the observations by Bundesland and then within every Bundesland by "GRÜNE_percent"; in descending order because of the minus sign.

## Creating new variables

With the command `mutate()` you can create new variables in the data frame. Suppose we want to see how the two parties that are currently in a coalition at the national level, ÖVP and GRÜNE, did in the EU elections 2014. So, we want to create a new variable, "COAL_percent" which is the sum of "ÖVP_percent" and "GRÜNE_percent". To limit the output, we also reduce the number of columns using the previously discussed instruments. Since there is no need for saving the converted data frame, we pipe it to `glimpse()` at the end.

```{r}
EU_elections %>% 
  select(1:2, contains("percent")) %>% 
  mutate(COAL_percent = ÖVP_percent + GRÜNE_percent) %>% 
  arrange(-COAL_percent) %>% 
  glimpse()
```

# Some data visualizations

This section is only intended for demonstration purposes. Some of the functions and data requirements are demanding and therefore too time consuming in this general workshop. Graphing and mapping with R would be a good topic for a more specialized workshop.

## Preparing data

We want to look at the EU election data by state ("Bundesland") and by district ("Bezirk"). First, we extract the respective rows from the data-frame. By inspecting the data-frame, we see that the GKZ-column of all state rows ends with "0000". So, we can filter by this condition.

```{r}
bl <- EU_elections %>% 
  filter(str_ends(GKZ, "0000")) %>% 
  filter(!str_ends(GKZ, "00000"))
```

For the districts, we need to filter all rows that end on "00", but, eliminate those that end with four zeros (Austria and state totals), ald those where the district id is not numeric. for the last criterion we first extract the district id fromthe variable `GKZ` with `str_sub()`. It is between position 2 and 4. Then, we convert the result to a number with `as.numeric()`. When this is not a number, the result is `NA`. Therefore, in the final step, with `!is.na()` we select only those rows where the result is not `NA`.

In the last line, we generate a variable `g_id` with the (numeric) district id. Note that this variable has the same name as the district id in `bez`.

```{r}
bez <- EU_elections %>% 
  filter(!is.na(as.numeric(str_sub(GKZ, 2,4))))  %>% 
  filter(!str_ends(GKZ, "0000")) %>% 
  filter(str_ends(GKZ, "00")) %>% 
  mutate(g_id = str_sub(GKZ, 2, 4))
```

## The simple `plot` function

Although `ggplot2` has become the standard tool for graphing in R, the simple `plot` function is still useful for quick views of the data.

First, we want to plot the variables "ÖVP_percent" and "SPÖ_percent". We select these two variables from the dataframe and pipe them to the `plot` function:

```{r}
bez %>% 
  select("ÖVP_percent", "SPÖ_percent") %>% 
  plot()
```

Then, we are asked to add the results for FPÖ to our output. When we just add "FPÖ_percent" to our `select` statement, this is what we get:

```{r}
bez %>% 
  select("ÖVP_percent", "SPÖ_percent", "FPÖ_percent") %>% 
  plot()
```

The result is a **matrix of scatterplots** for all the pairs of variables. The corresponding plots above and below the main diagonal are identical, just flipped.

We may then want to see all parties in one matrix. For that, we select the variables that contain "percent" (as above)

```{r}
bez %>% 
  select(contains("percent")) %>% 
  plot()
```

For the plot function, we always need at least two variables. When we provide just one, the output is not what we expect.

```{r}
bez %>% 
  select("SPÖ_percent") %>%  
  plot()
```

We need to add a sequence and probably want to plot the data by size. Below, we sort the data with `arrange` and then add a variable named "sequence" with values running from 1 to the number of rows in the dataframe (`n()`). We change the plot type to "l" for lines. If we want to see the observations as well, we can use "b" ("both") instead.

```{r}
bez %>% 
  select("SPÖ_percent") %>% 
  arrange(SPÖ_percent) %>% 
  mutate(sequence = 1:n()) %>% 
  plot(type = "l")
```

## A glimpse of `ggplot2`

In recent years, `ggplot2` has become something like the standard tool for graphing in R. This package is loaded with the `tidyverse` package and it uses the "geometry of graphics" approach for the creation of graphics. The tool is very powerful (see e.g., [this Video](https://youtu.be/ZrkjRwsnj6Y?si=QGqSlVoeMbTKqBLp)), but needs more time to explain. Therefore, I just give a glimpse of the tool.

### A bar chart

First, we want to plot the number of voters in the Bundesländer in a vertical bars graph.

```{r}
bl %>% 
  ggplot(aes(Name, Voters)) +
  geom_col()
```

We want the bars in a different color. With `color` we can set the outline of the bars, with `fill` the color of the filling.

```{r}
bl %>% 
  ggplot(aes(Name, Voters)) +
  geom_col(color="darkblue", fill="blue")
```

### A Scatterplot

```{r}
bez %>% 
  ggplot(aes(ÖVP_percent, SPÖ_percent)) +
  geom_point()
```

We can overlay a regression line that shows the average relation between the two variables. By default, a confidence interval is added. When we specify `se = NULL`, the confidence interval is dropped.

```{r}
bez %>% 
  ggplot(aes(ÖVP_percent, SPÖ_percent)) +
  geom_point() +
  geom_smooth(method = "lm")
```

### Maps

For a more comprehensive introduction see [@mieno2023].

We load the package `sf`. It implements the "Simple Feature" concept in R.

We read the shapefile for the Austrian districts into `bez_map`. When we inspect it, we see that there is a column with the district ID and one called `geometry` with the outlines of each of the districts.

```{r}
library(sf)

bez_map <- st_read("OGDEXT_POLBEZ_1_STATISTIK_AUSTRIA_20230101/STATISTIK_AUSTRIA_POLBEZ_20230101.shp")
```

To see what we got, we select the variable `geometry` and plot it. We get the outlines of the districts.

```{r}
bez_map  %>%  
  select(geometry)  %>%  
  plot()
```

Now, we merge the data in `bez` with the geographical information in `bez_map`. Since we used the same name for the district id, the function uses these values for merging. Then, we pipe the result to the function `st_as_sf()` to convert to "Simple Format".

```{r}
bez_full <- merge(bez_map, bez)  %>% 
  st_as_sf()
```

Now, we are ready to plot the map. In the `aes()` function we specify that the areas should be filled according to the values of `GRÜNE_percent`. `geom_sf()` does the mapping, `scale_fill_viridis_c()` sets the color schema.

```{r}
ggplot(bez_full, aes(fill=GRÜNE_percent)) +
  geom_sf() +
  scale_fill_viridis_c()
```

Strangely, by default the lowest values are assigned the darkest colors. To reverse this, we set the parameter `direction` to -1.

```{r}
ggplot(bez_full, aes(fill=GRÜNE_percent)) +
  geom_sf() +
  scale_fill_viridis_c(direction = -1)
```

{{< pagebreak >}}

# Some basic statistical analyses

The EU election data is not very well suited for the types of analysis I want to discuss. Therefore, I want to use the dataset "diamonds" that we loaded with the package "tidyverse". You may want to look into the dataset with `View(diamonds)`.

## Summary statistics

One of the most used functions in R is `summary()`. It is a generic function that produces a summary of the supplied object. Depending on the class of the object, the output of `summary()` looks quite different.

When we call `summary()` with a data frame, the function prints summary statistics for all the variables in the data frame. Here is the result for the data frame "diamonds".

```{r}
summary(diamonds)
```

Note the difference between "cut", "color", and "clarity" on the one hand and all the other variables on the other. The members of the second group are continuous variables. They can take on a range of values. The numbers we find in the variable "price", for example, represent the prices of the various diamonds. "cut", "color", and "clarity", on the other hand, are categorical variables. They categorize the observations. In R such variables are usually stored as factors with labels and values. The variable "cut", for example, has labels "Fair", "Good", etc. Internally, each of these categories is stored with a specific number. All diamonds with a "Good" cut, for example, may internally have the value 2 stored in this variable.

As you can see in the result above, the output of `summary()` differs for the two types of variables. For continuous variables, the function shows the lowest and highest values, the mean, the median, and the first and third quartile. For categorial variables where those indicators make no sense, the function lists the categories and how many observations fall into each of them (absolute frequencies).

## Grouped summaries

We would like to get some of the statistics that we saw above for the whole dataset broken down by categories. The package "tidyverse" offers tools for that. With the function `group_by()` we can define categories in our data frame. The function `summarize()` (different from `summary()` above) then applies some analysis to each group separately.

Suppose we want to get the mean and the standard deviation of the price for each of the "cut" categories. The following set of commands does exactly that. You can use any number of parameters for `summarize()`.

```{r}
diamonds  %>%  
  group_by(cut)   %>%   
  summarize(mean(price), sd(price))
```

We can define more appropriate headings for the columns:

```{r}
diamonds %>% 
  group_by(cut) %>% 
  summarize(av_price = mean(price), sd_price = sd(price))
```

The special function `n()` gives the number of observations in each group.

```{r}
diamonds %>% 
  group_by(cut) %>% 
  summarize(av_price = mean(price), sd_price = sd(price), count = n())
```

When we use more than one categorical variable in the `group_by()` command, we get more and smaller groups. The grouping is first done for the first variable, then within each group for the second variable, and so on.

```{r}
diamonds %>% 
  group_by(cut, color) %>% 
  summarize(av_price = mean(price), sd_price = sd(price), count = n())
```

## Crosstabulation

We want to know whether the categories "cut" and "color" are related. Let us first create a two-way table.

```{r}
table(diamonds$cut, diamonds$color)
```

Since the command `table()` does not expect the data frame as the first parameter, we cannot use a pipe. Instead, we need to inform the function that our variables are in "diamonds". This is done with `dataframe$variable`.

Alternatively, we can `attach()` the data frame "diamonds". When we do this, R will first look for variables in this dataframe. To undo this connection, use the command `detach()`.

```{r}
attach(diamonds)
table(cut, color)
```

Instead of just printing the table, we should store it in an object and then work with this object. When we print the object, we get the same output as above.

```{r}
mytable <- table(cut, color)
mytable
```

With `summary()`, we get meta information about the table and the results of a Chi-square test.

```{r}
mytable <- table(cut, color)
summary(mytable)
```

Instead of the absolute frequencies, we may want to see relative frequencies. Also, we can request the Chi-square test explicitly from the table object.

```{r}
mytable <- table(diamonds$cut, diamonds$color)
prop.table(mytable)
chisq.test(mytable)
```

To reduce the number of digits in the table, wrap it in the `round()` function and supply the number of digits you want.

```{r}
mytable <- table(diamonds$cut, diamonds$color)
round(prop.table(mytable),4)
```

## Analysis of Variance

We want to check whether or not the three categorical variables significantly influence the price of diamonds. The appropriate method is Analysis of Variance, called by `aov()` in R.

For the analysis of variance, we have to specify a model formula. This is the same as in regression analysis. We use a very simple version of the model formula. One can specify very different and quite complex models with this tool.

Our model formula says: use "price" as the dependent variable and "cut", "color" and "clarity" as explanatory variables. Note the "\~" between "price" and the other variables and that the explanatory variables are combined with "+".

```{r}
aov(price ~ cut + color + clarity)
```

When we use the `aov()` command, the output is limited. We can get more detailed information with `summary()`. We store the result of `aov()` in an object and supply that to `summary()`.

```{r}
anova <- aov(price ~ cut + color + clarity)
summary(anova)
```

We see that all three categorical variables significantly influence the price.

## Regression Analysis

We want to know how carat and the categories influence the price of diamonds. We run a linear regression with the command `lm()`. We expand the above model formula with "carat".

```{r}
lm(price ~ carat + cut + color + clarity)
```

When we run this, we get again very limited output. We get more information with the `summary()` function (we store the result of the regression analysis in the object "reg" and supply that to `summary()`).

```{r}
reg <- lm(price ~ carat + cut + color + clarity)
summary(reg)
```

For every categorical variable there is one coefficient less estimated than the number of categories. I do not know (and could not find out) why the names of the categories look like that. In a real application, I would need to do that. Since we do not interpret the results, it does not really matter here.

Anyways, we see that most regression coefficients are highly significant. From the t-value and significance indicator of "carat" we see that this variable significantly contributes to explaining the price.

### Is the contribution of a categorical variable significant?

For each of the categorical variables we get a number of regression coefficients. Most of them are significant, but not all. To check whether or not a categorical variable, say "color", as a whole is significant, we can run a likelihood ratio test. For that, we first run a regression with this variable dropped from the model formula

```{r}
reg.2 <- lm(price ~ carat + cut + clarity)
summary(reg.2)
```

Now, we have two objects with regression results stored; "reg" and "reg.2". The package "lmtest" provides a function for likelihood ratio tests (`lrtest()`).

```{r}
library(lmtest)
lrtest(reg.2, reg)
```

The result shows that the six color variables together have a significant impact on the price.

### Checking the residuals

An important aspect of regression analysis is to check the residuals of the "reg" model. The residuals are included in the "reg" object and can be accessed via `reg$residuals`. First, we calculate the mean and the standard deviation of the residuals, the minimum and the maximum.

```{r}
(reg.mean <- mean(reg$residuals))
(reg.sd <- sd(reg$residuals))
min(reg$residuals)
max(reg$residuals)
```

In a second step, we plot a histogram of the residuals and overlay it with a normal density function with the mean and standard deviation that we calculated above.

```{r}
hist(reg$residuals, breaks=50, freq=FALSE)
curve(dnorm(x, mean=reg.mean, sd=reg.sd), 
      col="darkblue", lwd=2, add=TRUE)
```

{{< pagebreak >}}

# Writing up and publishing your analysis

When we do quantitative analysis, one of the problems is to keep track of our workflow. What dataset did we use for what step? After all the different trials, what is our final model version? Did we exclude observations from the analysis and which ones? Such questions can be particularly troubling when we work with one of those easy to use menu driven statistics programs.

At a more general level, these questions raise issues of reproducibility and replicability that are discussed in the scientific literature.

R and RStudio offer various tools for keeping track of your workflow. I cannot go into much detail here. I will just sketch the main issues.

## Saving the history

A very valuable tool in this context is the "history" that R collects. Whenever you submit a command to R via the console, R adds this command to the bottom of the history. To inspect the history, go to the History pane in the Environment window.

With the "disk" icon in the tools bar of the History pane you can save the whole history to a file. You can do the same also from the console with the command `savehistory()`. The saved history is plain text. So you can give it the extension ".txt" or ".R", if you want to develop an R-script from it.

## R scripts

As mentioned before, you can collect R commands in R scripts either manually or by saving the history. R scripts have the file extension ".R". You can run the script (i.e., all the included commands in sequence) from the command line of your operating system with the program "Rscript". Alternatively, you can open the script in RStudio (File - Open File) and then execute it via the Source button. That is equivalent to typing `source("<path to script>")`in the console. For example, the following line runs the script "myscript.R" and echoes the commands in the script and their output:

```{r}
source("myscript.R", echo=TRUE)
```

A third option for running a script is the function `Rscript()` in the package "xfun".

When you just collect the R commands in your script, as you get when you save the history, you will not achieve much improvement in terms of reproducibility and replicability. Without any further information, it will be difficult to understand your workflow after a few months. Comments can resolve some of these issues. I suggest that you structure your scripts with empty lines and comments that explain what is the purpose of the following set of commands. In an R script, everything following a `#` is a comment. You can add lines or blocks of lines of comments and append comments to individual commands.

## RMarkdown and Quarto

When you properly comment R scripts, you may reach a point where the comments become something like the backbone of a report, an article, or even a book. Instead of adding text in the form of comments to a set of R commands, you may want to add R commands to some text that you prepare. It would be great if you could even run the R commands from your text and embed their results into your text.

This is exactly what you can achieve with RMarkdown and Quarto. Quarto is a more comprehensive version of RMarkdown. For our purpose, however, we do not have to worry about their differences. We will just talk about Quarto.

According to the Quarto homepage, Quarto is

> An open-source scientific and technical publishing system

Quarto [@tierney2022] is a standard that allows you to combine formatted text with R code. Text formatting uses a markdown dialect. The syntac is quite simple. A line starting with one hash (`#`) followed by a blank, for example, creates a top level heading. Two hashes create a second level header, three a third level, and so on. One or more blank lines separate the paragraphs. Text enclosed in single asteriscs (`*text*`) is set in italics, double asteriscs (`**text**`) make the text bold. The markdown dialect of Quarto lets you include footnotes, figures, hyperlinks, references, and mathematical expressions using LaTeX syntax.

Here are a few examples using LaTeX mathematics:

-   pure LaTeX (equation environment)

```{=tex}
\begin{equation}
E = mc^2
\end{equation}
```
-   pure LaTeX style (eqnarray environment)

```{=tex}
\begin{eqnarray}
Y & \sim & X\beta_0 + X\beta_1 + \epsilon \\
\epsilon & \sim & N(0,\sigma^2) \nonumber
\end{eqnarray}
```
-   Quarto style (double dollar signs)

$$
Y \sim X\beta_0 + X\beta_1 + \epsilon
$$

$$
\epsilon \sim N(0,\sigma^2)
$$

-   Mathematics in the text: In the statement above, $Y$ is the dependent variable, $X$ the independent variable. $\beta_0$ and $\beta_1$ are coefficients that we want to estimate. All the random influence is symbolized by $\epsilon$.

### Code chunks and inline code

A unique feature of Quarto and RMarkdown are "code chunks". Code chunks are snippets of R code embedded in your document. Usually, this code is executed by R and the result is inserted into the document underneath the code chunk.

Code chunks start with ```` ```{r} ```` and end with ```` ``` ````. Everything between those markers is considered R code or a code chunk option. With code chunk options you can determine, for example, whether the R code should be shown and whether it should be executed. Quarto defines a large number of code chunk options.

You can also embed code directly into your text. The syntax for that is the following:

```{r}
#| eval: false
`r <code>`
```

This is most useful to extract specific results from the analysis. For example, we can use

```{r}
#| eval: false
`r reg$coefficients["carat"]` 
```

to extract the estimate for the variable "carat" from the first regression. This could be used in the following text, for example: Between the full and the constrained regression, the coefficient of "carat" changes by `r round(reg.2$coefficients["carat"] - reg$coefficients["carat"], 2)` units from `r round(reg$coefficients["carat"],2)` to `r round(reg.2$coefficients["carat"],2)`.

### How to use a Quarto document

Quarto documents are identified with the file extension ".qmd". You can start, open, and edit Quarto documents in the Source window of RStudio, Whenever a Quarto document is open and active in RStudio, your tools will look somewhat different. In addition to a Source pane you will also see a Visual pane. When you switch to that pane, you will see the formatted version of your text. The visual pane is also an editor that helps you with the markdown syntax.

Irrespective whether you use the visual or the text editor, each code chunk will be shown with a light gray background and with two buttons on the top right. Clicking the first one (a downward pointing triangle with a green bar) will execute all code chunks above the current one. Clicking the second one (a green triangle pointing right) will execute the current chunk. When a code chunk generates output, it will be inserted after the code chunk.

You do not have to click through all the code chunks. The button "Run" in the toolbar offers a set of options. When you click the last one("Run All"), RStudio will go through your Quarto document and execute all code chunks in sequence. You can follow the execution in the Console. Any warnings or error messages will also be displayed there.

In the toolbar there is also a button "Render". When you click this button, RStudio will execute all the code chunks and in addition generate some output from your Quarto document. Some possible output formats are **HTML**, **PDF**, **Word**, **OpenOffice**, and **ePub**. Which format to use is specified in the so called YAML-code at the top of a Quarto document. There you can specify a set of parameters that let you generate books, articles, presentations, blog entries, web pages, and so on. For the purpose of illustration, I will first produce a HTML-page and then a PDF-file with the same content.

{{< pagebreak >}}

# Conclusion

This presentation provides a quick overview of R, RStudio, and some of their exciting new features like RMarkdown and Quarto. None of the aspects was covered comprehensively. Practically all R commands that I have mentioned offer more parameters that influence their behavior. We could go deeper in almost every aspect. Options that come to my mind are

-   statistical procedures
-   R data types
-   graphics with R (esp. "ggplot")
-   writing R functions
-   details of the markdown language
-   specifics of Quarto
-   challenges of reproducibility and replicability

{{< pagebreak >}}

# References

::: {#refs}
:::
